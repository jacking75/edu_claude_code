# 에이전트 팀을 1주일 실무에 투입하고 나서 "내 일의 정의"가 바뀌었다  
  
[출처](https://zenn.dev/neurostack_0001/articles/agent-teams-one-week-redefine-work)  
  

## **Agent Teams란?:**
Agent Teams는 **복수의 Claude Code 인스턴스가 하나의 팀으로 협조 동작하는 구조**다. 기존의 Claude Code는 1개의 세션에서 1개의 AI와 대화하는 방식이었지만, Agent Teams에서는 "리드 + 복수의 팀원"이라는 구성으로, 각 멤버가 독립된 컨텍스트를 가지면서 병렬로 작업한다.

**아키텍처**

| 요소 | 역할 |
|---|---|
| **Team lead** | 메인 세션. 팀 생성·태스크 할당·성과 통합을 담당 |
| **Teammates** | 독립된 컨텍스트 윈도우를 가진 워커 인스턴스. 각각이 전문 영역을 담당 |
| **Task list** | 멤버 전원이 참조하는 공유 태스크 리스트. pending → in-progress → completed로 진행 상황 관리 |
| **Mailbox** | 멤버 간의 직접 메시징. 리뷰 요청이나 질문을 멤버끼리 주고받음 |

포인트는 각 멤버가 **독립된 컨텍스트 윈도우**를 갖는다는 것이다. 인간의 팀 개발과 마찬가지로, 구현 담당이 묵묵히 코드를 작성하는 동안 리뷰어가 다른 파일을 체크하고, 테스트 설계자가 테스트 전략을 세운다 — 이것이 **진짜로 병렬로 진행된다.**

**서브에이전트와의 차이**

Claude Code에는 이전부터 "서브에이전트"(`.claude/agents/`에서 정의하는 커스텀 에이전트)가 있지만, Agent Teams와는 근본적으로 다른 구조다.

|  | 서브에이전트 | Agent Teams |
|---|---|---|
| **커뮤니케이션** | 부모에게 결과를 돌려줄 뿐 (일방향) | **멤버끼리 Mailbox로 직접 주고받음** |
| **태스크 관리** | 부모가 모두 관리·할당 | **공유 태스크 리스트로 자기 조정** |
| **컨텍스트** | 부모 세션 안에서 동작 | **각 멤버가 독립된 컨텍스트 윈도우** |
| **커스터마이즈** | `.claude/agents/`에 Markdown으로 사전 정의 | **프롬프트로 자연어 방식으로 동적 생성** |
| **병렬성** | 순차 실행 (백그라운드 실행은 가능) | **진정한 병렬 실행** |
| **메모리** | `memory` 필드로 세션을 넘어서는 축적 가능 | 세션 내에서만 (영속화 없음) |
| **안정성** | 안정 버전 | 실험적 기능 |
| **토큰 비용** | 낮음 | 높음 (각 멤버가 독립 인스턴스) |

**활용 구분의 기준:**

- **서브에이전트에 적합**: 결과만 필요한 정형 태스크 (파일 탐색, 단일 리뷰, 테스트 실행 등)
- **Agent Teams에 적합**: 멤버 간의 논의·협조가 필요한 복잡한 작업 (기능 개발, 리팩토링, 병렬 리뷰 등)

이 글에서는 **Agent Teams + Skills**의 조합에 집중해서, 1주일의 실험 결과를 전달한다.


## **실험 설계: 3체 에이전트 팀:**
이번에는 이 3체 구성으로 1주일을 돌렸다.

- **Implementer(구현 담당)**: 코드를 작성하는 주력
- **Reviewer(리뷰어)**: 보안·퍼포먼스 관점으로 리뷰
- **Tester(테스트 설계자)**: 테스트 전략 수립과 구현

단순히 "3인 팀을 만들어"라고 지시하는 것이 아니라, **Skills**로 팀원의 판단 기준을 정비하고, **Agent Teams**로 자연어로 팀 구성을 지시한다 — 이 조합이 포인트다.

**Step 1: Skills로 리뷰 기준과 테스트 전략을 정의한다**

CLAUDE.md에 전부 쓰면 비대해지고, 팀원마다 필요한 지식이 다르다. **Skills**를 사용하면 필요한 지식만 정리해서 공유할 수 있다. Skills는 Agent Teams의 팀원에게도 계승되기 때문에, **팀 전체의 공통 자산**이 된다.

**`.claude/skills/review-checklist/SKILL.md`**:

```markdown
---
name: review-checklist
description: 코드 리뷰의 기준 체크리스트
user-invocable: false
---

## 리뷰 체크리스트

### 보안
- [ ] 유저 입력의 유효성 검사·살균
- [ ] SQL 인젝션 대책 (파라미터 바인딩)
- [ ] XSS 대책 (출력 이스케이프)
- [ ] 인증·인가 체크의 누락
- [ ] 기밀 정보의 하드코딩 (API 키, 패스워드)

### 퍼포먼스
- [ ] N+1 쿼리의 유무
- [ ] 불필요한 리렌더링 (React: useCallback/useMemo)
- [ ] 대용량 데이터의 비효율적인 처리 (메모리 내 정렬 등)
- [ ] 인덱스 없는 DB 쿼리

### 유지보수성
- [ ] 함수·변수 명명이 의도를 나타내는가
- [ ] 1함수가 50행을 초과하지 않는가
- [ ] 매직 넘버의 배제
- [ ] 중복 코드의 유무
```

**`.claude/skills/test-strategy/SKILL.md`**:

```markdown
---
name: test-strategy
description: 테스트 설계의 방침과 기준
user-invocable: false
---

## 테스트 설계 방침

### 커버리지 목표
- 유닛 테스트: 80% 이상
- 통합 테스트: 주요 유스케이스를 모두 커버
- E2E 테스트: 크리티컬 패스만

### 우선해야 할 테스트 관점
1. 정상 계통의 주요 플로
2. 유효성 검사 에러 (입력의 경계값)
3. 인증·인가의 경계 (미로그인, 권한 부족)
4. 비동기 처리의 에러 핸들링
5. Race condition이 발생할 수 있는 부분

### 테스트 작성 방법
- Arrange-Act-Assert 패턴으로 통일
- 테스트명은 "무엇을 하면 무엇이 일어나는가"를 한국어로 작성
- 모킹은 최소한으로 (외부 API·DB 접속만)
```

`user-invocable: false`로 설정하는 것이 포인트인데, 이것은 유저가 `/review-checklist`로 호출하는 것이 아니라 **에이전트에게 자동 주입되는 전문 지식**으로 사용한다. Agent Teams의 팀원에게는 프로젝트의 Skills로 계승된다.

**Step 2: Agent Teams를 활성화하고 기동한다**

`settings.json`에서 Agent Teams를 활성화한다.

```json
{
  "env": {
    "CLAUDE_CODE_EXPERIMENTAL_AGENT_TEAMS": "1"
  }
}
```

그리고 Agent Teams를 기동하는 프롬프트:

```
에이전트 팀을 생성해서, 인증 모듈의 리팩토링에 임해주세요.

3명의 팀원을 생성:
- 구현 담당: src/auth/ 하위의 리팩토링. JWT 처리의 간소화와 에러 핸들링의 통일을 담당
- 리뷰어: 시니어 코드 리뷰어로 행동한다. review-checklist Skill에 따라, 보안(OWASP Top 10)·퍼포먼스(N+1, 메모리 리크)·유지보수성의 3관점으로 리뷰. Critical / Warning / Suggestion의 3단계로 보고할 것
- 테스트 설계자: 테스트 설계 전문가로 행동한다. test-strategy Skill에 따라, 유닛 테스트→통합 테스트→E2E의 우선순으로 테스트 전략을 수립·구현한다

구현 담당의 태스크를 이하와 같이 분할:
1. JWT 검증 로직의 공통화
2. 에러 응답의 통일
3. 토큰 리프레시 처리 개선
4. 로그아웃 처리의 간소화
5. 타입 정의 정리

각 멤버는 서로의 진행 상황을 공유하고, 문제가 있으면 논의해주세요.
리뷰어는 plan approval을 필수로 하며, 구현 전에 리뷰를 통과할 것.
```



## **Day 1-3: 움직이기 시작하고 나서 보인 Skills의 위력:**

**Skills가 효과를 발휘한 순간**

가장 "오!"라고 느낀 것은, 리뷰어가 Skills의 체크리스트에 맞춰 **구조화된 리뷰를 돌려왔을 때**다.

```
## Critical
- src/auth/login.ts:42 - 패스워드 해시 비교가 타이밍 공격에 취약
  → crypto.timingSafeEqual()을 사용해주세요

## Warning
- src/auth/token.ts:15 - JWT의 유효 기간 24h는 너무 길다
  → 액세스 토큰은 15분, 리프레시 토큰은 7일을 권장

## Suggestion
- src/auth/middleware.ts:8 - 인증 미들웨어의 함수명 `check`가 모호하다
  → `requireAuthentication`으로 변경을 제안
```

Skills를 주입하지 않았을 때는 "전체적으로 괜찮습니다. 몇 가지 개선점이 있습니다"라는 식의 모호한 리뷰였던 것이, **체크리스트가 있는 것만으로 구체성이 완전히 달라졌다.**


## **Day 4-5: 팀워크의 최적화:**
**delegate mode로 인간은 "방향만 결정한다"**

에이전트끼리의 논의가 활발해지면, 인간이 전체를 파악하기 어려워진다. 여기서 효과를 발휘한 것이 **delegate mode**(Shift+Tab)다.

리드 에이전트를 조율 역할에 전념시키고, 인간은 **Shift+Up/Down**으로 멤버를 선택해서 직접 지시를 내린다. 이 운용 방식으로 바꾸고 나서 "인간 = 태스크를 실행하는 사람"에서 "인간 = 방향을 결정하고 품질을 판단하는 사람"으로 완전히 이행할 수 있었다.

**병렬 리뷰의 Skill화**

PR 리뷰가 특히 효과적이었기 때문에, 반복 사용할 수 있도록 Skill화했다.

**`.claude/skills/parallel-review/SKILL.md`**:

```markdown
---
name: parallel-review
description: PR을 3가지 관점에서 병렬 리뷰하는 에이전트 팀을 기동한다
disable-model-invocation: true
---

PR을 리뷰하는 에이전트 팀을 생성해주세요.

3명의 리뷰어를 생성:
- 보안 담당: OWASP Top 10의 관점으로 리뷰
- 퍼포먼스 담당: N+1 쿼리, 메모리 리크, 불필요한 리렌더링을 체크
- 테스트 담당: 테스트 커버리지 검증, 엣지 케이스 누락을 지적

대상: $ARGUMENTS

각 리뷰어는 review-checklist Skill의 기준에 따라, Critical / Warning / Suggestion의 3단계로 보고할 것.
리뷰 완료 후, 리드가 3가지 관점을 통합해서 서머리를 작성해주세요.
```

이것으로 `/parallel-review #142`라고 입력하는 것만으로 3관점의 병렬 리뷰가 실행된다. `disable-model-invocation: true`로 설정했기 때문에, **내가 명시적으로 호출했을 때만** 기동한다.

**Agent Teams의 주요 제한 사항 (작성 시점 기준)**

- **세션 재개 불가**: in-process로 생성된 Teammates는 세션 종료와 함께 소멸한다. 도중에 재개는 불가능하다
- **1세션 1팀**: 동시에 복수의 팀을 생성·운용하는 것은 불가능하다
- **네스트 불가**: Teammates(멤버)가 스스로 팀을 만드는 것은 불가능하다. 팀을 만들 수 있는 것은 리드뿐이다
- **리드 고정**: 리드 에이전트의 승격·이양은 불가능하다. 리드는 항상 팀을 생성한 에이전트다
- **VS Code Terminal 등에서의 창 분할 비대응**: Agent Teams의 UI는 Claude Code CLI에서의 이용을 전제로 하고 있다

---

## **Day 6-7: 회고 — 인간이 해야 할 일은 무엇이었나:**

1주일 해보고, 인간의 일이 명확하게 바뀌었다.

| 지금까지 | 앞으로 |
|---|---|
| 코드를 작성한다 | **Agent Teams의 팀 구성을 설계하고, Skills로 판단 기준을 정의한다** |
| 리뷰 기준을 말로 전달한다 | **Skills에 체크리스트를 작성한다** |
| 혼자서 순서대로 리뷰한다 | **Agent Teams로 병렬 실행시키고 결과를 선택한다** |

특히 실감한 것은, "**많이 만드는 힘**"에서 "**어떤 미래를 선택하는지를 설계하는 힘**"으로의 전환이다. AI가 3가지 구현안을 내놓는다. 인간은 "어느 것을 선택할 것인가", "왜 그것을 선택하는가"를 판단한다. 이 **"선택하는 힘"**이야말로, 에이전트 시대에 가장 가치를 갖는 스킬이라고 느꼈다.


## **내일부터 시작하는 2단계:**
**Step 1: Skills에 판단 기준을 빼낸다**

CLAUDE.md에 전부 쓰는 것을 그만두고, Skills로 분리한다. **필요한 지식을 정리해서 공유**함으로써, 팀원의 아웃풋 품질이 완전히 달라진다.

```
.claude/
└── skills/
    ├── review-checklist/
    │   └── SKILL.md      # 리뷰 기준
    └── test-strategy/
        └── SKILL.md      # 테스트 방침
```

**Step 2: Agent Teams로 병렬화한다**

Skills가 안정되면, Agent Teams로 병렬화해 보자. 팀원은 **프롬프트로 자연어 방식으로 역할을 지시해서 동적으로 생성**한다. Step 1에서 정비한 Skills는 팀원에게도 계승되기 때문에 그대로 활용할 수 있다. 반복해서 사용하는 패턴은 Skill화(`/parallel-review` 등)해두면 커맨드 하나로 기동할 수 있다.


## **정리:**
1주일의 실험에서 배운 것은 단순하다.

- **CLAUDE.md에만 의존하지 않는다.** `.claude/skills/`에 판단 기준을 빼내면, 팀원 전원이 같은 품질 기준으로 움직일 수 있다
- **Agent Teams는 "인간이 선택하기 위한" 구조다.** 병렬로 움직이게 하고, 인간은 결과를 보고 판단한다

바뀐 것은 툴이 아니라 "내 일의 정의"였다. 코드를 작성하는 사람에서, **팀의 전문성을 설계하고 결과를 선택하는 사람**으로. 처음에는 당황스럽지만, 이 체험은 불가역적이다.

  
## **참고 링크:**
- [Claude Code Agent Teams 공식 문서](https://code.claude.com/docs/en/agent-teams)
- [Skills 공식 문서](https://code.claude.com/docs/en/skills)